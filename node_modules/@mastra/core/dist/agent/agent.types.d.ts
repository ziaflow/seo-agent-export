import type { TelemetrySettings } from 'ai';
import type { ModelMessage, ToolChoice } from 'ai-v5';
import type { TracingContext, TracingOptions } from '../ai-tracing/index.js';
import type { SystemMessage } from '../llm/index.js';
import type { StreamTextOnFinishCallback, StreamTextOnStepFinishCallback } from '../llm/model/base.types.js';
import type { MastraLanguageModel } from '../llm/model/shared.types.js';
import type { LoopConfig, LoopOptions, PrepareStepFunction } from '../loop/types.js';
import type { InputProcessor, OutputProcessor } from '../processors/index.js';
import type { RuntimeContext } from '../runtime-context/index.js';
import type { MastraScorer, MastraScorers, ScoringSamplingConfig } from '../scores/index.js';
import type { OutputSchema } from '../stream/base/schema.js';
import type { ChunkType } from '../stream/types.js';
import type { MessageListInput } from './message-list/index.js';
import type { AgentMemoryOption, ToolsetsInput, ToolsInput, StructuredOutputOptions } from './types.js';
export type MultiPrimitiveExecutionOptions = {
    /** Memory configuration for conversation persistence and retrieval */
    memory?: AgentMemoryOption;
    /** Unique identifier for this execution run */
    runId?: string;
    /** Runtime context containing dynamic configuration and state */
    runtimeContext?: RuntimeContext;
    /** Maximum number of steps to run */
    maxSteps?: number;
    /** AI tracing context for span hierarchy and metadata */
    tracingContext?: TracingContext;
    /** Model-specific settings like temperature, maxTokens, topP, etc. */
    modelSettings?: LoopOptions['modelSettings'];
    telemetry?: TelemetrySettings;
};
export type AgentExecutionOptions<OUTPUT extends OutputSchema = undefined, FORMAT extends 'mastra' | 'aisdk' | undefined = undefined> = {
    /**
     * Determines the output stream format. Use 'mastra' for Mastra's native format (default) or 'aisdk' for AI SDK v5 compatibility.
     * @default 'mastra'
     */
    format?: FORMAT;
    /** Custom instructions that override the agent's default instructions for this execution */
    instructions?: SystemMessage;
    /** Custom system message to include in the prompt */
    system?: SystemMessage;
    /** Additional context messages to provide to the agent */
    context?: ModelMessage[];
    /** Memory configuration for conversation persistence and retrieval */
    memory?: AgentMemoryOption;
    /** Unique identifier for this execution run */
    runId?: string;
    /** Save messages incrementally after each stream step completes (default: false). */
    savePerStep?: boolean;
    /** Runtime context containing dynamic configuration and state */
    runtimeContext?: RuntimeContext;
    /** @deprecated Use memory.resource instead. Identifier for the resource/user */
    resourceId?: string;
    /** @deprecated Use memory.thread instead. Thread identifier for conversation continuity */
    threadId?: string;
    /** Telemetry collection settings for observability */
    telemetry?: TelemetrySettings;
    /** Maximum number of steps to run */
    maxSteps?: number;
    /** Conditions for stopping execution (e.g., step count, token limit) */
    stopWhen?: LoopOptions['stopWhen'];
    /** Provider-specific options passed to the language model */
    providerOptions?: LoopOptions['providerOptions'];
    /** Callback fired after each execution step. Type varies by format */
    onStepFinish?: FORMAT extends 'aisdk' ? StreamTextOnStepFinishCallback<any> : LoopConfig['onStepFinish'];
    /** Callback fired when execution completes. Type varies by format */
    onFinish?: FORMAT extends 'aisdk' ? StreamTextOnFinishCallback<any> : LoopConfig['onFinish'];
    /** Callback fired for each streaming chunk received */
    onChunk?: LoopConfig<OUTPUT>['onChunk'];
    /** Callback fired when an error occurs during streaming */
    onError?: LoopConfig['onError'];
    /** Callback fired when streaming is aborted */
    onAbort?: LoopConfig['onAbort'];
    /** Tools that are active for this execution */
    activeTools?: LoopConfig['activeTools'];
    /** Signal to abort the streaming operation */
    abortSignal?: LoopConfig['abortSignal'];
    /** Input processors to use for this execution (overrides agent's default) */
    inputProcessors?: InputProcessor[];
    /** Output processors to use for this execution (overrides agent's default) */
    outputProcessors?: OutputProcessor[];
    /** Additional tool sets that can be used for this execution */
    toolsets?: ToolsetsInput;
    /** Client-side tools available during execution */
    clientTools?: ToolsInput;
    /** Tool selection strategy: 'auto', 'none', 'required', or specific tools */
    toolChoice?: ToolChoice<any>;
    /** Model-specific settings like temperature, maxTokens, topP, etc. */
    modelSettings?: LoopOptions['modelSettings'];
    /** Evaluation scorers to run on the execution results */
    scorers?: MastraScorers | Record<string, {
        scorer: MastraScorer['name'];
        sampling?: ScoringSamplingConfig;
    }>;
    /** Whether to return detailed scoring data in the response */
    returnScorerData?: boolean;
    /** AI tracing context for span hierarchy and metadata */
    tracingContext?: TracingContext;
    /** AI tracing options for starting new traces */
    tracingOptions?: TracingOptions;
    /** Callback function called before each step of multi-step execution */
    prepareStep?: PrepareStepFunction<any>;
    /** Require approval for all tool calls */
    requireToolApproval?: boolean;
} & OutputOptions<OUTPUT>;
type OutputOptions<OUTPUT extends OutputSchema = undefined> = {
    /**
     * Schema for structured output generation (Zod schema or JSON Schema)
     * @deprecated Use `structuredOutput` instead. The `output` property will be removed in a future version.
     */
    output?: OUTPUT;
    structuredOutput?: never;
} | {
    /** Structured output generation with enhanced developer experience  */
    structuredOutput?: StructuredOutputOptions<OUTPUT extends OutputSchema ? OUTPUT : never>;
    output?: never;
};
export type InnerAgentExecutionOptions<OUTPUT extends OutputSchema = undefined, FORMAT extends 'aisdk' | 'mastra' | undefined = undefined> = AgentExecutionOptions<OUTPUT, FORMAT> & {
    writableStream?: WritableStream<ChunkType>;
    messages: MessageListInput;
    methodType: 'generate' | 'stream' | 'generateLegacy' | 'streamLegacy';
    /** Internal: Model override for when structuredOutput.model is used with maxSteps=1 */
    model?: MastraLanguageModel;
    /** Internal: Whether the execution is a resume */
    resumeContext?: any;
};
export {};
//# sourceMappingURL=agent.types.d.ts.map